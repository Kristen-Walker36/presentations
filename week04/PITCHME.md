# How do we know what we see?

19 September 2016

---

## Course Map



Note:

---

## Outline

1. How does seeing work?
1. What is visual perception?
1. How do we construct meaning from visual information?
1. How do we think of things we can't see or touch?

---

## How does seeing work?

- Illusions
- The inverse problem
- Visual stimuli
- Visual system

Note:

Much of the material in this segment is augmented by thinking captured from Dale Purvis' (2010) book "Brains: How they seem to work".

Foundationally, brain and behavior scientists agree that that brain extracts features from sensory stimuli and then combine them in representations in higher cortical areas. These are the signals we perceive and use.

According to Purvis, the problem is that stimuli are very noisy and undestanding how the brain takes sensory signals and generates perceptions is unclear.

We'd like to say that there is a straightforward process that begins with the detection of features in the retina and moves toward a representation in the cerebral cortex.

The real problem is context -- as we will see. So let's take a look at that. I'll show you several examples: luminance, color, geometry, and angles.

---

## Luminance 

<img src="images/luminance.png" align="center" height="100%" width="100%">

*Our vision does not work like a camera*

Note:

The inner circle has the same luminance in both color contexts. (Luminance is "brightness")

---

## Color

<img src="images/cube.png" align="center" height="100%" width="100%">

Note: 

Note the center tile on the top and the center one on the facing side.

---

## Color

<img src="images/brown.png" align="center" height="80%" width="80%">

Note: Again, you perceive the two tiles differently because the visual context differs.
---

## Geometry

<img src="images/tables.png" align="center" height="100%" width="100%">
---

## Geometry

<img src="images/geometry.png" align="center" height="100%" width="100%">

---

## Angles

.center[![:scale 100%](images/angles.png)]

Note:

The visual world is unknowable: we can't actually see the world like a camera. Purvis (2010) believes we evolved perception to behavioral feedback due to operational successes in evolutionary time.

---

## Visual Spectrum

<img src="images/electromagnetic-spectrum.png" align="center" height="100%" width="100%">
*We see only a tiny portion of the electromagnetic spectrum*

Note:

We have the illusion of believing we see everything there is to see. This, coupled with the observations above should cause you to question how vision works.

---

## Visual Stimuli

<img src="images/photon-flux.png" align="center" height="100%" width="100%">
.
*What reaches the eye is not an image but a (chaotic) photon flux*

** Somehow the eye has evolved to make an image on the retina**

Note:

What's different between an eye and a camera?
The heavy lifting is not done by the lens but the cornea.

---
## Visual System

<img src="images/cornea.png" align="center" height="100%" width="100%">

*The cornea alters the speed and direction of light rays*

Note:

Since the cornea is heavier than air, when light hits its curved surface, it is refracted.

The lens refines the light a bit earlier.

A primary feature of central vision is the macula lutea (color, blood vasculature difference than the rest of the retina). The **fovea** has the greatest detail and finest resolution of the eye. To see, we need to make sure we align our eyes to a point of fixation so that objects falls on the fovea. In this way, we can see detail and color. The fovea is the area of greatest sensitivity.

---

## Fovea

<img src="images/foveal.png" align="center" height="100%" width="100%">

Note:

Ware notes that at arms length, we can resolve 100 points on the head of a pin. But on the edge of our visual field, we can only resolve something the size of a human head.

---

## Foveal resolution

<img src="images/fovea-text.png" align="center" height="100%" width="100%">

Note:

In order to see, you have to move your eyes continually.

The Russian physiologist, Alfred Yarbas, was the first to do eye tracking. He used a contact lens with a mirror on it and subjects would look at a scene. (You can see what his equipment looked like here! https://en.m.wikipedia.org/wiki/Eye_tracking)

What are our eyes doing when looking at a scene normally? They are making **saccadic eye movements** as we look at scenes on a continual basis. This is a very jerky movement. Our eyes move for periods around 50 ms and when they do, we are actually blind.

Our eye movements are not random. For example, people look at informational characteristics of a face and spend less time on in-informative regions. This is completely unconscious.

---

## Retina

<img src="images/retinal-image.png" align="center" height="90%" width="90%">

*The retina is the interface between the non-neural and neural parts of the eye.*

Note:

The retina sits at the back of the eye. It has a layer of cells that are photosensitive. 

Let's look at why they are at the back of the eye.

---

## Photoreceptors

<img src="images/photopigment-rods-cones.png" align="center" height="80%" width="80%">

Note:

 70% of all human receptors are visual. Receptors are protein molecules that receives a signal (in this case chemical) and produce a response. 
 
Photoreceptors contain photopigments that react to  light. Some change their shape when they absorb a photon. When photopigments change shape, they trigger chemical reactions that send signals to the visual cortex indicating how much light there is at some point on the retina. Thus, light is received as electromagnetic radiation in waves and is turned into  nerve impulses that the brain understands. 

There are a number of specialized cells in the retina. Aside from photoreceptor cells, there are also horizontal cells, bipolar cells, amacrine cells, and ganglion cells.  They all perform certain kinds of functions. For example, horizontal cells help process information in a lateral direction, as well as in light and dim conditions.

---

## Distribution of rods and cones

<img src="images/dist-rods-cones-retina.png" align="center" height="80%" width="80%">

Note:

 Different types of photoreceptors contain different kinds of photopigment -- the main kinds are known as rods and cones. Rod cells are much more numerous except for in the area of the fovea (center of the macula lutea). Rods are more sensitive than cones and we rely on them when there is little light. Cones, on the other hand, enable us to see distinguish color.

Though, modern humans are accustomed to more light than our ancestors, presumably from an evolutionary standpoint, we needed both types of photoreceptors to accommodate a range of conditions.

---

## Photon capture of rods and cones

<img src="images/retina-rods-cones.png" align="center" height="80%" width="80%">

Note:

Both rods and cones are efficient at capturing photons. Rods converge on ganglion cells through intermediary processors. About 100 converge on a single ganglion cell. It takes about 100 rods to detect a single flash of light.
Cones capture more photons and converge on ganglion cells at about 1:1.

---

## Luminance range

<img src="images/luminance-rods-cones.png" align="center" height="100%" width="100%">

Note:

Luminance is a measure of light emission or reflection. In this figure, you can see that it encompasses orders of magnitude.

---

## Primary visual pathway

<img src="images/primary-visual-pathway.png" align="center" height="60%" width="60%">

Note:

This is the route from the eye to what we perceive. The superior colliculus is responsible for the coordination of eye movements.

We really don't know why there is hemisphere crossing. Perhaps it is evolutionary?

---

## The inverse problem

<img src="images/inverse-problem.jpg" align="center" height="70%" width="70%">

*Problem with vision as feature representation (Purvis, 2015)*

Note:

Basically, this is called "the inverse problem" because we are trying to calculate cause from results.

In early Hubel and Wiesel studies in the 1950's and 1960's, neurons in the primary visual pathway of cats and monkeys were found to respond to light stimuli in specific ways. They discovered that neurons on the V1 part of the cortex responded selectively to retinal activation (bars with different orientation, etc.). This
led to they theory that the visual system operates in a straight-forward way of feature extraction, filtering, processing and analytic combination so that an approximate representation
of physical reality guides our behavior in the world: we see the world as it is.

Purvis suggests this is wrong. He motivates the need for a paradigmatic shift by pointing out "the inverse problem". He believes that our perceptions are derived empirically by association between sensory patterns and the relative success of behavior in response to patterns. 

Purves, Morgenstern, Wojtach, 2015, "Perception and Reality: Why a Wholly Empirical Paradigm is Needed to Understand Vision", Front. Syst. Neurosci., 18

---

## Visual thinking

"Visual thinking consists of a series of acts of attention, driving eye movements and tuning our pattern finding circuits. These acts of attention are called visual queries."

*Colin Ware, Visual Thinking for Design, 2011, p. 128-129*

---

## What does Ware mean by "visual queries"?

<img src="images/visual-query.png" align="center" height="90%" width="90%">

Note:

Eye movement is characterized by saccadic eye movements (when vision is suppressed) and fixations.

We perceive during fixations, which last on the order of .1-.2 seconds.

Thus, we sample the environment to see and can only see a small amount at a time. Information not critical is not retained.

---

## Illusion: All is in conscious awareness

<iframe width="560" height="315" src="https://www.youtube.com/embed/IGQmdoK_ZfY" frameborder="0" allowfullscreen></iframe>

Note:

.1 seconds seems instantaneous. We can't hold the whole world in conscious awareness.

Change blindness reflects our small capacity of visual working memory. This is the reason we need external visual aids. Our attention is displayed by task-relevant data.

---

## Seeing is attention

Working memory resources are used to briefly retain in focal attention those resources that are needed

---

## Feature detection

<iframe width="560" height="315" src="https://www.youtube.com/embed/RPv0a9ftu6Y" frameborder="0" allowfullscreen></iframe>

[Hubel and Wiesel cat experiment](https://www.youtube.com/watch?v=IOHayh06LJ4)

Note:

Basically, Hubel and Wiesel discovered that there are different sorts of detectors in the eye. Some are specialized or edge detection, some for motion detection, some for color, depth, etc. Kittens that did not receive certain kinds of visual signals during early development were not able to develope them later.

---

## Feature detection (summary)

<img src="images/feature-detection.png" align="center" height="90%" width="90%">

While there are 100M visual receptors, there are billions of neurons simultaneously processing edges, contours, shapes, color, motion and depth.

Note:

- Strongest pop-out affects when a single target objects differs in some feature from all other identical. Degree of contrast important. The more variable the background, the larger the difference. Color, orientation, size, motion & stereoscopic depth. Less than .1 sec.
- No pop out for 2 more features at the same time.
- Learning does not help much in this (hard-wired) phase.
- Detection field  (area around center of the fovea) matters - the differences must be close enough together in visual space.

---

## Color

<img src="images/ConeMosaics.jpg" align="center" height="90%" width="90%">

Note:

Three color opponent channels:
- red-green (middle and long wavelengths)
- yellow-blue (luminance and blue cones)
- black-white (luminance/light intensity)

We are designed to see differences between patches of light: not absolute values.
Color blindness (generally red-green): 8% of males, 1 % females affected

- Luminescent differences easier to see than chromatic differences.
	- luminescent channel conveys motion more effectively.
	- conveys stereoscopic depth.
	- conveys shape from shading difference.
	- it's is non-linear; we're more sensitive to dark grey differences than light differences. 
- We are more sensitive to properties of surfaces in the environment rather than light coming from those surfaces.
- We are attuned to color for search.

---

## Attentional tuning of features

<img src="images/visual-query.png" align="center" height="100%" width="100%">

 *Patterns are the power of vision*

Note:
Planning and executing eye movements occurs between 1-3 times per second.

For each fixation, a person can do between 1-4 pattern tests.

---

## Binding

<img src="images/contours.png" align="center" height="100%" width="100%">

Note: 

In this stage, patterns such as continuous contours are constructed. There is a biased competition for feature processing. For example, excitatory signals on neurons are prevalent within a neighborhood. Likewise, there tends to be inhibition on non-related inputs.

There are also top-down attentional processes (goals, actions, understanding, etc.) We are continually re-link visual and non-visual features; we see what we are looking for and we get information when we need it.

---

## Ames illusion

<iframe width="560" height="315" src="https://www.youtube.com/embed/gJhyu6nlGt8" frameborder="0" allowfullscreen></iframe>

---

## Perception

*Binding together of visual information with non-visual concepts is perception.*

---

## Top-down interaction

<iframe width="560" height="315" src="https://www.youtube.com/embed/UCy-Lc6hfFA" frameborder="0" allowfullscreen></iframe>

Note:

Ware notes that the brain acts like a distributed computer with no central conductor; different parts of the brain can go about doing different things through very loose coordination.

---

## Different people can perceive the world differently


  <iframe width="560" height="315" src="https://www.youtube.com/embed/jexnhNfOzHg" frameborder="0" allowfullscreen></iframe>

https://www.wired.com/2015/02/science-one-agrees-color-dress/

---

## Planning and reading

<iframe width="560" height="315" src="https://www.youtube.com/embed/VFIZDZwdf-0" frameborder="0" allowfullscreen></iframe>

---

## Challenges to reading

<iframe width="560" height="315" src="https://www.youtube.com/embed/TwNNij89qro" frameborder="0" allowfullscreen></iframe>

---

## Design tips for visual queries

1. Break down tasks visually.
1. It's often easier to re-do than remember.
1. Multi-scale visual structure makes search easier.
1. Closeness in visual space important.
1. For two or more objects in visual query, use different channels (e.g, size, color, orientation, motion, grouping, etc). Getting pop-out effects on more than 2-3 symbols can be challenging. 
1. Signaling icons should emerge and disappear periodically to reduce habituation.
1. Prior experience leads to pattern of eye movement tendencies. Scanning strategies are shaped by the predictability of the environment. Regardless, experts may find patterns more quickly on fixations, but still cannot find them out of the corner of one's eye.
1. Start with unique hues for color coding. Use no more than a dozen. Background matters.
1. Color is multi-dimensional and affected by context.
1. Remember that olors are often used symbolically in different cultures.
1. Shape perception are mainly based on luminance. (Useful in maps for revealing patterns.)
1. We don't see color peripherally, search tasks might be more effective with shape.
1. Luminance contrasts are especially useful for detail and small text.
1. Depth is inferred.

---

## "What hierarchy" - How objected are represented in the mind

<img src="images/what-hierarchy.png" align="center" height="90%" width="90%">

Note:

From features represented in V1 as very simple patterns (such as color, lines, etc.), as signals move from V1 to V2 more complex patterns are detected (e.g, borders of objects). Then as signals are propagated forward, still more patterns are "attuned" at higher levels. In V4, objects (as patterns) are recognized; this is a sort of information reduction. Once you have an object in mind, your brain fills in all sorts of information about what you expect to see. Objects can be stored in visual working memory. We can only hold about three objects in attention at a time -- and visual working memory lasts only seconds. But because we can bind non-visual associational information with visual information, top-down search processes are supported by expectations about objects we've perceived.

In this diagram, you can also see two post V1/V2 pathways: the "what" pathway is concerned with the identification of objects, while the "where" pathway keeps track of locations. Again, bias is very important here: we're biased not to re-visit things we've observed. Where pathways are also connected to representations of ourselves and our bodies.

---

## How do we construct meaning?

<img src="images/dog.png" align="center" height="70%" width="70%">

1. The visual world is unknowable: we don't see the world like a camera.
2. Cognition is distributed - top-down and bottom-up processes have an effect on perception.
3. Graphics are designed to communicate. Consider the visual task and allow that to drive design.
4. The power of visual thinking is pattern matching -- but patterns aren't limited to the visual - perceptions linked to actions are response patterns.
5. In pattern learning, V1 & V2 are universally the same, V4 is idiosyncratic based on experience.

---

## How do we think of things we can't touch?

*Cognition depends on aspects of our bodies and not just our brains.
We offload our cognitive processing onto the environment and distribute
across our physical, social and cultural environment. (Suchman, 1987)*

---

## Embodied cognition

*Thought is based on the way our bodies work*

<iframe width="560" height="315" src="https://www.youtube.com/embed/Eu-9rpJITY8" frameborder="0" allowfullscreen></iframe>

[Lakoff - Frames and metaphor](https://www.youtube.com/watch?v=S_CWBjyIERY)

Note:

Historically, in philosophy of mind & cognitive science considered the body to be peripheral to thinking. These days we tend to believe that our physicality is integral to thinking.

---

## Space

<img src="images/chair-table.jpg" align="center" height="90%" width="90%">

Note:

In English, we have three frames of reference.

- Object - The chair is in front of the table.
- Viewer (egocentric) - The chair is behind the table. (Egocentric - up, sideways, towards-away)
- Absolute - The chair is to the North of the table.

---

## Metaphor

<img src="images/apple.jpg" align="center" height="90%" width="90%">

Metaphors are patterns of patterns.

Note:

Spatial concepts like "front", "back", "up", "down" are articulated
in terms of our body's position and movement.

But, we use them to talk about abstract things:
- Happy is up, sad is down
- push a button/limit (abstract object)
- Halloween is coming up (moving objects; future events are up)
- Weeks ahead

Properties and functions of objects matter: in/on bowl/plate

---

## Visual imagery

<iframe width="560" height="315" src="https://www.youtube.com/embed/OTEa1jwJbqU" frameborder="0" allowfullscreen></iframe>

Note:

When you visualize an image, your eyes move a pattern that reflects that image (skip to min 6)

---

## Performance on cognitive tasks

We can perform cognitive tasks better when using our bodies or parts of the environment around us (Donald, 1991).

Note:

Perception of space is fundamentally about perception of action potential -- the linking of perception to action.
For example, softball players who are hitting better see the ball a bigger.

- Does ability influence perception or is there response bias (judgement bias from memory)?

---

## Mirror Neurons

<iframe width="560" height="315" src="https://www.youtube.com/embed/t0pwKzTRG5E" frameborder="0" allowfullscreen></iframe>

---

## How do we think of things we can't see or touch?

Perhaps, we borrow from sensory and motor representations as we interact with the world.

---

## References

Donald, M. (1991). Origins of the Modern Mind: Three Stages in the Evolution of Culture and Cognition. Harvard University Press.

Lakoff, G., & M. Johnson. (1999). Philosophy in the Flesh: The Embodied Mind and its Challenge to Western Thought, New York: Basic Books.

Purves, D. (2010). Brains: How They Seem to Work. Ft Press.

Purves D, Morgenstern Y, & Wojtach WT. (2015). Perception and reality: Why a wholly empirical paradigm is needed to understand vision. Front. Syst. Neurosci. 9:156.

Suchman, Lucy (1987). Plans and Situated Actions: The Problem of Human-machine Communication. Cambridge: Cambridge University Press.

Ware, C. (2010). Visual Thinking for Design. Morgan Kaufmann.

---

## Next Week

- Journal
- Elevator speech

Note:

2-minute summary of your case study problem.
